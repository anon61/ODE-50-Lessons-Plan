Episode 33: Matrix Exponential - Computing e^(At)**

Audio Lesson Script**

Welcome to Lesson 33! Today we're tackling the matrix exponential - this is the unifying concept that brings together everything we've learned about solving linear systems. So basically, the solution to any system x' = Ax with constant coefficients can be written as x(t) = e^(At)x₀. But how do we actually compute e^(At)? That's today's mission!

Here's the key insight: the matrix exponential is defined exactly like the scalar exponential, but with matrices:
**e^(At) = I + At + (At)²/2! + (At)³/3! + ...**

Now, you might think, "Oh no, an infinite series!" But here's the beautiful part - we have multiple clever ways to compute this without summing infinitely many terms. Let me show you the main methods.

**Method 1: Diagonalization (for diagonalizable matrices)**

If A = PDP^(-1) where D is diagonal with eigenvalues on the diagonal, then:
**e^(At) = Pe^(Dt)P^(-1)**

And here's the magic - e^(Dt) for a diagonal matrix is just the exponentials of the diagonal entries:
```
e^(Dt) = [e^(λ₁t)   0    ...  0   ]
         [0    e^(λ₂t)  ...  0   ]
         [...              ...   ]
         [0     0    ... e^(λₙt)]
```

Let's do a concrete example:
```
A = [3  1]
    [1  3]
```

We found in earlier lessons: λ₁ = 4, λ₂ = 2, with eigenvectors v₁ = [1,1]ᵀ and v₂ = [1,-1]ᵀ.

So P = [1  1], D = [4  0]
      [1 -1]      [0  2]

First, find P^(-1):
P^(-1) = (1/(-2))[−1  -1] = [1/2   1/2]
                  [-1   1]   [1/2  -1/2]

Therefore:
e^(At) = [1  1][e^(4t)   0  ][1/2   1/2]
         [1 -1][0     e^(2t)][1/2  -1/2]

Multiplying this out (I'll spare you the algebra):
e^(At) = [(e^(4t)+e^(2t))/2   (e^(4t)-e^(2t))/2]
         [(e^(4t)-e^(2t))/2   (e^(4t)+e^(2t))/2]

**Method 2: Jordan Form (for non-diagonalizable matrices)**

Remember those repeated eigenvalues with Jordan blocks? The exponential of a Jordan block is:
```
e^(Jt) = e^(λt)[1   t   t²/2  ...]
                [0   1   t     ...]
                [0   0   1     ...]
                [... ... ...   ...]
```

For example, if J = [λ  1], then e^(Jt) = e^(λt)[1  t]
                    [0  λ]                       [0  1]

This explains why we get those t terms in solutions with repeated eigenvalues!

**Method 3: Direct Series (for nilpotent or small matrices)**

If N is nilpotent (N^k = 0 for some k), the series terminates:
e^(Nt) = I + Nt + N²t²/2! + ... + N^(k-1)t^(k-1)/(k-1)!

Example: N = [0  1  0]
            [0  0  1]
            [0  0  0]

Since N³ = 0, we get:
e^(Nt) = I + Nt + N²t²/2 = [1  t  t²/2]
                            [0  1  t   ]
                            [0  0  1   ]

**Method 4: Cayley-Hamilton Theorem**

This is Prof. Ditkowski's favorite theoretical method! The Cayley-Hamilton theorem says every matrix satisfies its own characteristic equation. This means we can express higher powers of A in terms of lower powers, which lets us write e^(At) as a polynomial in A!

For a 2×2 matrix with characteristic polynomial p(λ) = λ² - tr(A)λ + det(A), we can show:
e^(At) = α(t)I + β(t)A

where α(t) and β(t) are determined by the eigenvalues.

**Method 5: For 2×2 Matrices - Direct Formula**

For a 2×2 matrix A with eigenvalues λ₁ ≠ λ₂:
e^(At) = (e^(λ₁t) - e^(λ₂t))/(λ₁ - λ₂) · A + (λ₁e^(λ₂t) - λ₂e^(λ₁t))/(λ₁ - λ₂) · I

For repeated eigenvalue λ:
e^(At) = e^(λt)[I + t(A - λI)]

Here's my memory trick: "DJNCS" - **D**iagonalization, **J**ordan, **N**ilpotent, **C**ayley-Hamilton, **S**pecial formulas.

Now, why is this matrix exponential so important? Because it gives us the complete solution operator! The solution to x' = Ax with x(0) = x₀ is simply:
**x(t) = e^(At)x₀**

This works for ALL cases - distinct eigenvalues, repeated eigenvalues, complex eigenvalues - everything!

Common mistakes to avoid:
1. **Matrix multiplication order matters** - Pe^(Dt)P^(-1) ≠ P^(-1)e^(Dt)P
2. **Don't compute infinitely many terms** - Use the clever methods!
3. **Check your answer** - e^(A·0) should equal I
4. **For Jordan blocks, powers of t appear** - Don't forget the factorials!

Properties to remember:
- e^(A·0) = I (always)
- d/dt[e^(At)] = Ae^(At) = e^(At)A
- e^(A(t+s)) = e^(At)e^(As)
- (e^(At))^(-1) = e^(-At)
- det(e^(At)) = e^(tr(A)t)

Prof. Ditkowski loves to test: "Compute e^(At) where A is a 2×2 Jordan block" or "Find e^(At) using diagonalization." He also asks conceptual questions like "Why does e^(At) always exist?" (Answer: the series always converges for finite matrices).

The physical meaning is beautiful: e^(At) is the state transition matrix. It tells you exactly how the system evolves from any initial state to any future time. It's THE fundamental object in linear system theory!